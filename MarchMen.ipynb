{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "pd.set_option(\"display.max_column\", 999)\n",
    "\n",
    "# Data preparation. \n",
    "\n",
    "tourney_results = pd.read_csv('C:/Users/Bryan/Desktop/2023 March Madness Prediction Project/Data/MNCAATourneyDetailedResults.csv')\n",
    "seeds = pd.read_csv('C:/Users/Bryan/Desktop/2023 March Madness Prediction Project/Data/MNCAATourneySeeds.csv')\n",
    "regular_results = pd.read_csv('C:/Users/Bryan/Desktop/2023 March Madness Prediction Project/Data/MRegularSeasonDetailedResults.csv')\n",
    "kp = pd.read_csv('C:/Users/Bryan/Desktop/2023 March Madness Prediction Project/Data/kenpom_2023.csv')\n",
    "\n",
    "regular_results['WEFFG'] = regular_results['WFGM'] / regular_results['WFGA']\n",
    "regular_results['WEFFG3'] = regular_results['WFGM3'] / regular_results['WFGA3']\n",
    "regular_results['WDARE'] = regular_results['WFGM3'] / regular_results['WFGM']\n",
    "regular_results['WTOQUETOQUE'] = regular_results['WAst'] / regular_results['WFGM']\n",
    "\n",
    "regular_results['LEFFG'] = regular_results['LFGM'] / regular_results['LFGA']\n",
    "regular_results['LEFFG3'] = regular_results['LFGM3'] / regular_results['LFGA3']\n",
    "regular_results['LDARE'] = regular_results['LFGM3'] / regular_results['LFGM']\n",
    "regular_results['LTOQUETOQUE'] = regular_results['LAst'] / regular_results['LFGM']\n",
    "regular_results.head()\n",
    "\n",
    "def prepare_data(df):\n",
    "    dfswap = df[['Season', 'DayNum', 'LTeamID', 'LScore', 'WTeamID', 'WScore', 'WLoc', 'NumOT', \n",
    "    'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', \n",
    "    'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']]\n",
    "\n",
    "    dfswap.loc[df['WLoc'] == 'H', 'WLoc'] = 'A'\n",
    "    dfswap.loc[df['WLoc'] == 'A', 'WLoc'] = 'H'\n",
    "    df.columns.values[6] = 'location'\n",
    "    dfswap.columns.values[6] = 'location'    \n",
    "      \n",
    "    df.columns = [x.replace('W','T1_').replace('L','T2_') for x in list(df.columns)]\n",
    "    dfswap.columns = [x.replace('L','T1_').replace('W','T2_') for x in list(dfswap.columns)]\n",
    "\n",
    "    output = pd.concat([df, dfswap]).reset_index(drop=True)\n",
    "    output.loc[output.location=='N','location'] = '0'\n",
    "    output.loc[output.location=='H','location'] = '1'\n",
    "    output.loc[output.location=='A','location'] = '-1'\n",
    "    output.location = output.location.astype(int)\n",
    "    \n",
    "    output['PointDiff'] = output['T1_Score'] - output['T2_Score']\n",
    "    \n",
    "    return output\n",
    "\n",
    "regular_data = prepare_data(regular_results)\n",
    "tourney_data = prepare_data(tourney_results)\n",
    "\n",
    "# Feature engineering!\n",
    "\n",
    "regular_data.columns\n",
    "\n",
    "\n",
    "boxscore_cols = ['T1_Score', 'T2_Score', \n",
    "        'T1_FGM', 'T1_FGA', 'T1_FGM3', 'T1_FGA3', 'T1_FTM', 'T1_FTA', 'T1_OR', 'T1_DR', 'T1_Ast', 'T1_TO', 'T1_Stl', 'T1_Blk', 'T1_PF', \n",
    "        'T2_FGM', 'T2_FGA', 'T2_FGM3', 'T2_FGA3', 'T2_FTM', 'T2_FTA', 'T2_OR', 'T2_DR', 'T2_Ast', 'T2_TO', 'T2_Stl', 'T2_Blk', 'T2_PF', \n",
    "        'PointDiff']\n",
    "\n",
    "boxscore_cols = [\n",
    "        'T1_FGM', 'T1_FGA', 'T1_OR', 'T1_Ast', 'T1_TO', 'T1_Stl', 'T1_PF', 'T1_FTM', 'T2_FTM', 'T2_FGM', 'T2_FGA', \n",
    "        'T2_OR', 'T2_Ast', 'T2_TO', 'T2_Stl', 'T2_Blk', 'T1_Score', 'T2_Score', 'PointDiff',\n",
    "        'T1_EFFG', 'T1_EFFG3', 'T1_DARE', 'T1_TOQUETOQUE', 'T2_EFFG', 'T2_EFFG3', 'T2_DARE', 'T2_TOQUETOQUE']\n",
    "\n",
    "boxscore_cols = ['T1_Score', 'T2_Score', \n",
    "        'T1_FGM', 'T1_FGA', 'T1_FGM3', 'T1_FGA3', 'T1_FTM', 'T1_FTA', 'T1_OR', 'T1_DR', 'T1_Ast', 'T1_TO', 'T1_Stl', 'T1_Blk', 'T1_PF', \n",
    "        'T2_FGM', 'T2_FGA', 'T2_FGM3', 'T2_FGA3', 'T2_FTM', 'T2_FTA', 'T2_OR', 'T2_DR', 'T2_Ast', 'T2_TO', 'T2_Stl', 'T2_Blk', 'T2_PF', \n",
    "        'T1_EFFG', 'T1_EFFG3', 'T1_DARE', 'T1_TOQUETOQUE', 'T2_EFFG', 'T2_EFFG3', 'T2_DARE', 'T2_TOQUETOQUE']\n",
    "\n",
    "# After my analysis\n",
    "#boxscore_cols = ['PointDiff', 'T1_Blk', 'T2_Blk', 'T1_Ast', 'T2_Ast', 'T1_Stl', 'T2_Stl', 'T1_FGA', \n",
    "#                 'T2_FGA', 'T1_FGM', 'T2_FGM', 'T1_DR', 'T2_DR', 'T1_Score', 'T2_Score']\n",
    "\n",
    "\n",
    "# Choose a function to aggregate\n",
    "funcs = [np.mean]\n",
    "\n",
    "\n",
    "season_statistics = regular_data.groupby([\"Season\", 'T1_TeamID'])[boxscore_cols].agg(funcs).reset_index()\n",
    "season_statistics.columns = [''.join(col).strip() for col in season_statistics.columns.values]\n",
    "#Make two copies of the data\n",
    "season_statistics_T1 = season_statistics.copy()\n",
    "season_statistics_T2 = season_statistics.copy()\n",
    "\n",
    "season_statistics_T1.columns = [\"T1_\" + x.replace(\"T1_\",\"\").replace(\"T2_\",\"opponent_\") for x in list(season_statistics_T1.columns)]\n",
    "season_statistics_T2.columns = [\"T2_\" + x.replace(\"T1_\",\"\").replace(\"T2_\",\"opponent_\") for x in list(season_statistics_T2.columns)]\n",
    "season_statistics_T1.columns.values[0] = \"Season\"\n",
    "season_statistics_T2.columns.values[0] = \"Season\"\n",
    "\n",
    "\n",
    "tourney_data = tourney_data[['Season', 'DayNum', 'T1_TeamID', 'T1_Score', 'T2_TeamID' ,'T2_Score']]\n",
    "tourney_data.head()\n",
    "\n",
    "tourney_data = pd.merge(tourney_data, season_statistics_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_data = pd.merge(tourney_data, season_statistics_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "# Notice that there are Team 1 statistics, team 1 opponent's statistics, team 2 statistics and team 2 opponent statistics\n",
    "tourney_data.head()\n",
    "\n",
    "# Cut the opponent columns that I don't want\n",
    "#opplist = [opp for opp in tourney_data.columns if '_opponent_' in opp]\n",
    "#todelete = [opp for opp in opplist if 'Blk' not in opp]\n",
    "#tourney_data.drop(todelete, axis = 1, inplace = True)\n",
    "#tourney_data.head()\n",
    "\n",
    "\n",
    "# These statistics are created because in the last 2 weeks some stuff may happen (injuries just before the tournament and such)\n",
    "#last14days_stats_T1 = regular_data.loc[regular_data.DayNum>118].reset_index(drop=True)\n",
    "#last14days_stats_T1['win'] = np.where(last14days_stats_T1['PointDiff']>0,1,0)\n",
    "#last14days_stats_T1 = last14days_stats_T1.groupby(['Season','T1_TeamID'])['win'].mean().reset_index(name='T1_win_ratio_14d')\n",
    "\n",
    "#last14days_stats_T2 = regular_data.loc[regular_data.DayNum>118].reset_index(drop=True)\n",
    "#last14days_stats_T2['win'] = np.where(last14days_stats_T2['PointDiff']<0,1,0)\n",
    "#last14days_stats_T2 = last14days_stats_T2.groupby(['Season','T2_TeamID'])['win'].mean().reset_index(name='T2_win_ratio_14d')\n",
    "\n",
    "#tourney_data = pd.merge(tourney_data, last14days_stats_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "#tourney_data = pd.merge(tourney_data, last14days_stats_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "\n",
    "# Extract the teams that make it to the tournament and see how they do with respect to the others\n",
    "regular_season_effects = regular_data[['Season','T1_TeamID','T2_TeamID','PointDiff']].copy()\n",
    "regular_season_effects['T1_TeamID'] = regular_season_effects['T1_TeamID'].astype(str)\n",
    "regular_season_effects['T2_TeamID'] = regular_season_effects['T2_TeamID'].astype(str)\n",
    "regular_season_effects['win'] = np.where(regular_season_effects['PointDiff']>0,1,0)\n",
    "march_madness = pd.merge(seeds[['Season','TeamID']],seeds[['Season','TeamID']],on='Season')\n",
    "march_madness.columns = ['Season', 'T1_TeamID', 'T2_TeamID']\n",
    "march_madness.T1_TeamID = march_madness.T1_TeamID.astype(str)\n",
    "march_madness.T2_TeamID = march_madness.T2_TeamID.astype(str)\n",
    "regular_season_effects = pd.merge(regular_season_effects, march_madness, on = ['Season','T1_TeamID','T2_TeamID'])\n",
    "regular_season_effects.shape\n",
    "\n",
    "## Team Quality\n",
    "\n",
    "def normalize_column(values):\n",
    "  themean = np.mean(values)\n",
    "  thestd = np.std(values)\n",
    "  norm = (values - themean)/(thestd) \n",
    "  return(pd.DataFrame(norm))\n",
    "\n",
    "def team_quality(season):\n",
    "    formula = 'win~-1+T1_TeamID+T2_TeamID'\n",
    "    glm = sm.GLM.from_formula(formula=formula, \n",
    "                              data=regular_season_effects.loc[regular_season_effects.Season==season,:], \n",
    "                              family=sm.families.Binomial()).fit()\n",
    "    quality = pd.DataFrame(glm.params).reset_index()\n",
    "    quality.columns = ['TeamID','quality']\n",
    "    quality['Season'] = season\n",
    "    quality['quality'] = normalize_column(quality['quality'])\n",
    "    # quality['quality'] = np.exp(quality['quality'])\n",
    "    quality = quality.loc[quality.TeamID.str.contains('T1_')].reset_index(drop=True)\n",
    "    quality['TeamID'] = quality['TeamID'].apply(lambda x: x[10:14]).astype(int)\n",
    "    print(quality['quality'].mean(), quality['quality'].std())\n",
    "    return quality\n",
    "\n",
    "# This is metric to measure the team's strength, in this case, this is a logistic regression and we\n",
    "# the coefficients\n",
    "glm_quality = pd.concat([team_quality(2003),\n",
    "                         team_quality(2004),\n",
    "                         team_quality(2005),\n",
    "                         team_quality(2006),\n",
    "                         team_quality(2007),\n",
    "                         team_quality(2008),\n",
    "                         team_quality(2009),\n",
    "                         team_quality(2010),\n",
    "                         team_quality(2011),\n",
    "                         team_quality(2012),\n",
    "                         team_quality(2013),\n",
    "                         team_quality(2014),\n",
    "                         team_quality(2015),\n",
    "                         team_quality(2016),\n",
    "                         team_quality(2017),\n",
    "                         team_quality(2018),\n",
    "                         team_quality(2019),\n",
    "                         team_quality(2021),\n",
    "                         team_quality(2022),\n",
    "                         team_quality(2023)]).reset_index(drop=True)\n",
    "\n",
    "glm_quality_T1 = glm_quality.copy()\n",
    "glm_quality_T2 = glm_quality.copy()\n",
    "glm_quality_T1.columns = ['T1_TeamID','T1_quality','Season']\n",
    "glm_quality_T2.columns = ['T2_TeamID','T2_quality','Season']\n",
    "\n",
    "tourney_data = pd.merge(tourney_data, glm_quality_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_data = pd.merge(tourney_data, glm_quality_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "tourney_data['T1_quality'].fillna(0.2, inplace = True)\n",
    "tourney_data['T2_quality'].fillna(0.2, inplace = True)\n",
    "tourney_data.T2_quality.isnull().sum()\n",
    "\n",
    "seeds['seed'] = seeds['Seed'].apply(lambda x: int(x[1:3]))\n",
    "seeds.head()\n",
    "\n",
    "seeds_T1 = seeds[['Season','TeamID','seed']].copy()\n",
    "seeds_T2 = seeds[['Season','TeamID','seed']].copy()\n",
    "seeds_T1.columns = ['Season','T1_TeamID','T1_seed']\n",
    "seeds_T2.columns = ['Season','T2_TeamID','T2_seed']\n",
    "\n",
    "tourney_data = pd.merge(tourney_data, seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_data = pd.merge(tourney_data, seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "#Optional but not relevant\n",
    "tourney_data[\"Seed_diff\"] = tourney_data[\"T1_seed\"] - tourney_data[\"T2_seed\"]\n",
    "\n",
    "massey = pd.read_csv('C:/Users/Bryan/Desktop/2023 March Madness Prediction Project/Data/MMasseyOrdinals_thru_Season2023_Day128.csv')\n",
    "\n",
    "# RANKINGS AVAILABLE\n",
    "massey[massey.RankingDayNum == 128].SystemName.unique()\n",
    "\n",
    "\n",
    "bagofRanks = dict()\n",
    "#oldtoconsider = ['WLK']\n",
    "trafalgars = ['WLK', 'SAG', 'POM', 'COL', 'DOL', 'MOR', 'RTH', 'WOL', 'ATP', 'EMK', 'DWH', 'AP']\n",
    "for traf in trafalgars:\n",
    "    bagofRanks[traf] = massey[(massey['SystemName']==traf) & (massey['RankingDayNum']==128)]\n",
    "    traf_T1 = bagofRanks[traf][['Season','TeamID','OrdinalRank']].copy()\n",
    "    traf_T2 = bagofRanks[traf][['Season','TeamID','OrdinalRank']].copy()\n",
    "    traf_T1.columns = ['Season','T1_TeamID','T1_OR_' + traf]\n",
    "    traf_T2.columns = ['Season','T2_TeamID','T2_OR_' + traf]\n",
    "    tourney_data = pd.merge(tourney_data, traf_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "    tourney_data = pd.merge(tourney_data, traf_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "    tourney_data[traf + \"_diff\"] = tourney_data[\"T1_OR_\" + traf] - tourney_data[\"T2_OR_\" + traf]\n",
    "    tourney_data.drop([\"T2_OR_\" + traf], axis = 1, inplace = True)\n",
    "\n",
    "# Time to build some models!\n",
    "\n",
    "# The descriptive feature is the score, not the winner\n",
    "y = tourney_data['T1_Score'] - tourney_data['T2_Score']\n",
    "y.describe()\n",
    "\n",
    "# Last chance to drop a couple of features:\n",
    "tourney_data.drop(['T1_OR_POM', 'T1_OR_RTH', 'T1_OR_WLK', 'T1_OR_COL', 'T1_OR_WOL', 'T1_OR_MOR'], axis = 1, inplace = True)\n",
    "# Drop own efficiency and OR - Curiously the opponent efficiency IS important. - Because we effectively damage it?\n",
    "tourney_data.drop(['T1_EFFGmean', 'T2_EFFGmean', 'T1_ORmean', 'T2_ORmean'], axis = 1, inplace = True)\n",
    "# This opponent data just seems to always be insignificant\n",
    "tourney_data.drop(['T1_opponent_Stlmean', 'T2_opponent_Stlmean', 'T1_opponent_Astmean', 'T2_opponent_Astmean', 'T1_opponent_Scoremean', 'T2_opponent_Scoremean', 'T1_opponent_FGMmean', 'T2_opponent_FGMmean'], axis = 1, inplace = True)\n",
    "features = tourney_data.columns[6:]\n",
    "# Drop the next ones from the features but not from the dataframe\n",
    "features.drop(['T2_seed'])\n",
    "len(features)\n",
    "\n",
    "\n",
    "X = tourney_data[features].values\n",
    "dtrain = xgb.DMatrix(X, label = y)\n",
    "\n",
    "\n",
    "# #Run the feature experiment to see their importance\n",
    "# from sklearn.model_selection import train_test_split \n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# X = tourney_data[features] \n",
    "# X['random_1'] = np.random.normal(0.0, 1.0, X.shape[0]) \n",
    "# X['random_2'] = np.random.normal(0.0, 1.0, X.shape[0]) \n",
    "# X['random_3'] = np.random.normal(0.0, 1.0, X.shape[0]) \n",
    "# def imp_df(column_names, importances):\n",
    "#     df = pd.DataFrame({'feature':column_names, 'feature_importance': importances}).sort_values('feature_importance', ascending = False).reset_index(drop = True)\n",
    "#     return(df)\n",
    "\n",
    "# myfeatures = dict() \n",
    "# for f in list(features) + ['random_1', 'random_2', 'random_3']: \n",
    "#     myfeatures[f] = list()\n",
    "    \n",
    "# for md in range(5,9): \n",
    "#     for n_estimators in [50, 55, 65, 75, 100]: \n",
    "#         for rs in range(6): \n",
    "#             clf = RandomForestClassifier(max_depth=md,n_estimators=n_estimators, random_state=rs) \n",
    "#             clf.fit(X, y) \n",
    "#             perm = PermutationImportance(clf, cv = None, refit = False, n_iter = 10).fit(X, y) \n",
    "#             perm_imp_eli5 = imp_df(X.columns, perm.feature_importances_) \n",
    "#             for c, f in enumerate([i for i in perm_imp_eli5['feature']]): \n",
    "#                 myfeatures[f].append(c) \n",
    "#             print('where is:', md, n_estimators, rs) \n",
    "#             print([i for i in perm_imp_eli5['feature']])\n",
    "                    \n",
    "# for f in list(features) + ['random_1', 'random_2', 'random_3']: \n",
    "#     print(f, myfeatures[f], max(myfeatures[f]), np.mean(myfeatures[f]), min(myfeatures[f]))\n",
    "\n",
    "# Loss function\n",
    "\n",
    "\n",
    "def cauchyobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    c = 5000 \n",
    "    x =  preds-labels    \n",
    "    grad = x / (x**2/c**2+1)\n",
    "    hess = -c**2*(x**2-c**2)/(x**2+c**2)**2\n",
    "    return grad, hess\n",
    "\n",
    "param = {} \n",
    "#param['objective'] = 'reg:linear'\n",
    "param['eval_metric'] =  'mae'\n",
    "param['booster'] = 'gbtree'\n",
    "param['eta'] = 0.02 #recommend change to ~0.02 for final run. Higher when debugging.\n",
    "param['subsample'] = 0.35\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['num_parallel_tree'] = 3 #recommend 10. Write 3 for debugging.\n",
    "param['min_child_weight'] = 40\n",
    "param['gamma'] = 10\n",
    "param['max_depth'] =  3\n",
    "param['silent'] = 1\n",
    "\n",
    "print(param)\n",
    "\n",
    "xgb_cv = []\n",
    "repeat_cv = 4 # recommend 10 for final submission. Smaller for debugging.\n",
    "\n",
    "for i in range(repeat_cv): \n",
    "    print(f\"Fold repeater {i}\")\n",
    "    xgb_cv.append(\n",
    "        xgb.cv(\n",
    "          params = param,\n",
    "          dtrain = dtrain,\n",
    "          obj = cauchyobj,\n",
    "          num_boost_round = 3000,\n",
    "          folds = KFold(n_splits = 5, shuffle = True, random_state = i),\n",
    "          early_stopping_rounds = 25,\n",
    "          verbose_eval = 50\n",
    "        )\n",
    "    )\n",
    "\n",
    "iteration_counts = [np.argmin(x['test-mae-mean'].values) for x in xgb_cv]\n",
    "val_mae = [np.min(x['test-mae-mean'].values) for x in xgb_cv]\n",
    "iteration_counts, val_mae\n",
    "\n",
    "#This is to get out-of-fold predictions\n",
    "oof_preds = []\n",
    "for i in range(repeat_cv):\n",
    "    print(f\"Fold repeater {i}\")\n",
    "    preds = y.copy()\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = i)    \n",
    "    for train_index, val_index in kfold.split(X,y):\n",
    "        dtrain_i = xgb.DMatrix(X[train_index], label = y[train_index])\n",
    "        dval_i = xgb.DMatrix(X[val_index], label = y[val_index])  \n",
    "        model = xgb.train(\n",
    "              params = param,\n",
    "              dtrain = dtrain_i,\n",
    "              num_boost_round = iteration_counts[i],\n",
    "              verbose_eval = 50\n",
    "        )\n",
    "        preds[val_index] = model.predict(dval_i)\n",
    "    oof_preds.append(np.clip(preds,-19,19))\n",
    "\n",
    "plot_df = pd.DataFrame({\"pred\":oof_preds[0], \"label\":np.where(y>0,1,0)})\n",
    "plot_df[\"pred_int\"] = plot_df[\"pred\"].astype(int)\n",
    "plot_df = plot_df.groupby('pred_int')['label'].mean().reset_index(name='average_win_pct')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_df.pred_int,plot_df.average_win_pct)\n",
    "\n",
    "\n",
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "        \n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    \n",
    "    print(f\"logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") \n",
    "\n",
    "plot_df = pd.DataFrame({\"pred\":oof_preds[0], \"label\":np.where(y>0,1,0), \"spline\":spline_model[0](oof_preds[0])})\n",
    "plot_df[\"pred_int\"] = (plot_df[\"pred\"]).astype(int)\n",
    "#This was working earlier but does not work for some reason\n",
    "#plot_df = plot_df.groupby('pred_int')['spline','label'].mean().reset_index()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_df.pred_int,plot_df.spline)\n",
    "plt.plot(plot_df.pred_int,plot_df.label)\n",
    "\n",
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    spline_fit = np.clip(spline_fit,0.025,0.975)\n",
    "    \n",
    "    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") \n",
    "\n",
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    spline_fit = np.clip(spline_fit,0.02,0.98)\n",
    "    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1)] = 0.0\n",
    "    \n",
    "    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") \n",
    "\n",
    "#looking for upsets\n",
    "pd.concat(\n",
    "    [tourney_data[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score < tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score < tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score > tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score > tourney_data.T2_Score)]]\n",
    ")   \n",
    "\n",
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    spline_fit = np.clip(spline_fit,0.025,0.975)\n",
    "    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==3) & (tourney_data.T2_seed==14) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==14) & (tourney_data.T2_seed==3) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    \n",
    "    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") \n",
    "\n",
    "val_cv = []\n",
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    spline_fit = np.clip(spline_fit,0.02,0.98)\n",
    "    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    \n",
    "    val_cv.append(pd.DataFrame({\"y\":np.where(y>0,1,0), \"pred\":spline_fit, \"season\":tourney_data.Season}))\n",
    "    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") \n",
    "    \n",
    "val_cv = pd.concat(val_cv)\n",
    "val_cv.groupby('season').apply(lambda x: log_loss(x.y, x.pred))\n",
    "\n",
    "# Submission time!\n",
    "\n",
    "sub = pd.read_csv('C:/Users/Bryan/Desktop/2023 March Madness Prediction Project/Data/SampleSubmission2023.csv')\n",
    "sub.head()\n",
    "\n",
    "sub.shape\n",
    "\n",
    "sub[\"Season\"] = sub[\"ID\"].apply(lambda x: x[0:4]).astype(int)\n",
    "sub[\"T1_TeamID\"] = sub[\"ID\"].apply(lambda x: x[5:9]).astype(int)\n",
    "sub[\"T2_TeamID\"] = sub[\"ID\"].apply(lambda x: x[10:14]).astype(int)\n",
    "sub.shape\n",
    "\n",
    "sub = pd.merge(sub, season_statistics_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "sub = pd.merge(sub, season_statistics_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "print(sub.shape)\n",
    "sub = pd.merge(sub, glm_quality_T1, on = ['Season', 'T1_TeamID'], how = 'left') # This is because some teams didn't face off in the regular season\n",
    "sub = pd.merge(sub, glm_quality_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "print(sub.shape)\n",
    "sub = pd.merge(sub, seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "sub = pd.merge(sub, seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "print(sub.shape)\n",
    "#sub = pd.merge(sub, last14days_stats_T1, on = ['Season', 'T1_TeamID'])\n",
    "#sub = pd.merge(sub, last14days_stats_T2, on = ['Season', 'T2_TeamID'])\n",
    "#print(sub.shape)\n",
    "sub[\"Seed_diff\"] = sub[\"T1_seed\"] - sub[\"T2_seed\"]\n",
    "sub.shape\n",
    "for traf in trafalgars:\n",
    "    traf_T1 = bagofRanks[traf][['Season','TeamID','OrdinalRank']].copy()\n",
    "    traf_T2 = bagofRanks[traf][['Season','TeamID','OrdinalRank']].copy()\n",
    "    traf_T1.columns = ['Season','T1_TeamID','T1_OR_' + traf]\n",
    "    traf_T2.columns = ['Season','T2_TeamID','T2_OR_' + traf]\n",
    "    sub = pd.merge(sub, traf_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "    sub = pd.merge(sub, traf_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "    sub[traf + \"_diff\"] = sub[\"T1_OR_\" + traf] - sub[\"T2_OR_\" + traf]\n",
    "sub.shape\n",
    "\n",
    "sub.columns\n",
    "\n",
    "sub.head()\n",
    "print(sub.T2_quality.isnull().sum())\n",
    "sub['T1_quality'].fillna(0.2, inplace = True)\n",
    "sub['T2_quality'].fillna(0.2, inplace = True)\n",
    "sub.T2_quality.isnull().sum()\n",
    "\n",
    "Xsub = sub[features].values\n",
    "dtest = xgb.DMatrix(Xsub)\n",
    "\n",
    "sub_models = []\n",
    "for i in range(repeat_cv):\n",
    "    print(f\"Fold repeater {i}\")\n",
    "    sub_models.append(\n",
    "        xgb.train(\n",
    "          params = param,\n",
    "          dtrain = dtrain,\n",
    "          num_boost_round = int(iteration_counts[i] * 1.05),\n",
    "          verbose_eval = 50\n",
    "        )\n",
    "    )\n",
    "\n",
    "sub_preds = []\n",
    "for i in range(repeat_cv):\n",
    "    sub_preds.append(np.clip(spline_model[i](np.clip(sub_models[i].predict(dtest),-30,30)),0.025,0.975))\n",
    "    \n",
    "sub[\"Pred\"] = pd.DataFrame(sub_preds).mean(axis=0)\n",
    "#sub[\"Pred\"] /= (2*sub[\"Pred\"].mean())\n",
    "sub.loc[sub['Pred'] > 0.5, \"Pred\"] *= 1.00 # This is calibrated by trial and error.\n",
    "sub.loc[sub['Pred'] < 0.5, \"Pred\"] /= 1.00\n",
    "print(sub['Pred'].mean())\n",
    "\n",
    "\n",
    "# Remove this before you submit on the other competition\n",
    "import statistics\n",
    "val = statistics.mode(sub.Pred)\n",
    "sub.loc[val == sub.Pred, 'Pred'] = 0.5\n",
    "statistics.mode(sub.Pred)\n",
    "\n",
    "#sub.loc[(sub.T1_seed==2) & (sub.T2_seed==15), 'Pred'] = 0.99\n",
    "#sub.loc[(sub.T1_seed==3) & (sub.T2_seed==14), 'Pred'] = 0.99\n",
    "#sub.loc[(sub.T1_seed==15) & (sub.T2_seed==2), 'Pred'] = 0.01\n",
    "#sub.loc[(sub.T1_seed==14) & (sub.T2_seed==3), 'Pred'] = 0.01\n",
    "#sub.loc[(sub.T1_seed==1) & (sub.T2_seed==16), 'Pred'] = 0.99\n",
    "#sub.loc[(sub.T1_seed==16) & (sub.T2_seed==1), 'Pred'] = 0.01\n",
    "sub[['ID','Pred']].to_csv(\"MarchMen.csv\", index = None)\n",
    "\n",
    "#tourney_results2018 = pd.read_csv('../input/NCAA_2018_Solution_Womens.csv')\n",
    "#tourney_results2018 = tourney_results2018[tourney_results2018.Pred!=-1].reset_index(drop=True)\n",
    "#tourney_results2018.columns = ['ID', 'label']\n",
    "#tourney_results2018 = pd.merge(tourney_results2018, sub, on = 'ID')\n",
    "#log_loss(tourney_results2018.label, tourney_results2018.Pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e28e4c851d8ebd910e2883edad9ea7020f295fa34c06f5da5752de2a09e6bf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
